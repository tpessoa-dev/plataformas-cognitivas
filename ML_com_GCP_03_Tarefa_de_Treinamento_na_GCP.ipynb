{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML com GCP - 03 - Tarefa de Treinamento na GCP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tpessoa-dev/plataformas-cognitivas/blob/main/ML_com_GCP_03_Tarefa_de_Treinamento_na_GCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzOInDaUmxrk"
      },
      "source": [
        "# Autenticação\n",
        "Primeiro precisamos autenticar nossa sessão do Colab no Google e definimos o ID do projeto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCN6GL5Umq9b",
        "outputId": "b930175f-80d1-4754-a219-c795369ed4e7"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsEtAA-3n1Q-"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Nesta seção vamos definir os necessários para o processamento na nuvem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZJPEinGn9gk"
      },
      "source": [
        "## Bibliotecas / pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zcGRBQ2myI4"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK3xaFjCoCYE"
      },
      "source": [
        "## Parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS2k7FyLn_4s"
      },
      "source": [
        "#@title Informe o id do projeto nesta linha e o bucket. Sugestão, coloque o nome do bucket como nome do projeto + um sufixo:\n",
        "PROJECT = 'curso-gcp-ml-tensorflow' #@param {type:\"string\"}\n",
        "BUCKET = 'curso-gcp-ml-tensorflow-17ia' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bjj3He_oElQ"
      },
      "source": [
        "#Outras variáveis:\n",
        "REGION = 'us-central1'\n",
        "\n",
        "#Variáveis do OS\n",
        "os.environ[\"PROJECT\"] = PROJECT\n",
        "os.environ[\"BUCKET\"] = BUCKET\n",
        "os.environ[\"REGION\"] = REGION\n",
        "os.environ[\"TFVERSION\"] = \"2.3\"\n",
        "os.environ[\"PYTHONVERSION\"] = \"3.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtzNX2OBoMjL",
        "outputId": "e372a498-62bc-4cbe-f492-25f11615f923"
      },
      "source": [
        "%%bash\n",
        "echo \"Your current GCP Project Name is: \"$PROJECT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your current GCP Project Name is: curso-gcp-ml-tensorflow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEv2S39zoqeR"
      },
      "source": [
        "# Training on Cloud AI Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suV9LJQxoygv"
      },
      "source": [
        "Para enviar para a nuvem, usamos [`gcloud ai-platform jobs submit training [jobname]`] (https://cloud.google.com/sdk/gcloud/reference/ml-engine/jobs/submit/training) e simplesmente especificamos alguns parâmetros adicionais para o AI Platform Training Service:\n",
        "- jobname: um identificador único para o trabalho na nuvem. Normalmente adicionamos o tempo do sistema para garantir a exclusividade\n",
        "- job-dir: um local GCS para fazer upload do pacote Python para\n",
        "- versão em tempo de execução: versão do TF a ser usada.\n",
        "- python-version: Versão do Python a ser usada. Atualmente, apenas Python 3.7 é compatível com TF 2.3.\n",
        "- região: região da nuvem para treinar. Consulte [aqui] (https://cloud.google.com/ml-engine/docs/tensorflow/regions) para regiões de serviço de treinamento do AI Platform compatíveis\n",
        "\n",
        "Abaixo de `-- \\` nós adicionamos os argumentos para nosso arquivo `task.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDEOJVFFo9ir"
      },
      "source": [
        "## Gera arquivos de treinamento (task.py e model.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdcgkqdForYF"
      },
      "source": [
        "%%bash\n",
        "mkdir ./pesobebes\n",
        "mkdir ./pesobebes/trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZrJRGDJpFV9",
        "outputId": "8ff5b6b8-1882-4931-b152-d6eee5315a18"
      },
      "source": [
        "%%writefile ./pesobebes/trainer/__init__.py\n",
        "# Arquivo em branco"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ./pesobebes/trainer/__init__.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02fEWWWdpMnz",
        "outputId": "8ab72ff1-72a2-44a6-955e-4ef4545f846b"
      },
      "source": [
        "%%writefile ./pesobebes/trainer/model.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import hypertune\n",
        "\n",
        "# Determine CSV, label, and key columns\n",
        "CSV_COLUMNS = [\"weight_kilos\",\n",
        "               \"is_male\",\n",
        "               \"mother_age\",\n",
        "               \"mother_married\",\n",
        "               \"plurality\",\n",
        "               \"gestation_weeks\"]\n",
        "LABEL_COLUMN = \"weight_kilos\"\n",
        "\n",
        "# Set default values for each CSV column.\n",
        "# Treat is_male and plurality as strings.\n",
        "DEFAULTS = [[0.0], [\"null\"], [0.0], [\"null\"], [\"null\"], [0.0]]\n",
        "\n",
        "\n",
        "def features_and_labels(row_data):\n",
        "    \"\"\"Splits features and labels from feature dictionary.\n",
        "    Args:\n",
        "        row_data: Dictionary of CSV column names and tensor values.\n",
        "    Returns:\n",
        "        Dictionary of feature tensors and label tensor.\n",
        "    \"\"\"\n",
        "    label = row_data.pop(LABEL_COLUMN)\n",
        "\n",
        "    return row_data, label  # features, label\n",
        "\n",
        "\n",
        "def load_dataset(pattern, batch_size=1, mode='eval'):\n",
        "    \"\"\"Loads dataset using the tf.data API from CSV files.\n",
        "    Args:\n",
        "        pattern: str, file pattern to glob into list of files.\n",
        "        batch_size: int, the number of examples per batch.\n",
        "        mode: 'train' | 'eval' to determine if training or evaluating.\n",
        "    Returns:\n",
        "        `Dataset` object.\n",
        "    \"\"\"\n",
        "    print(\"mode = {}\".format(mode))\n",
        "    # Make a CSV dataset\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        file_pattern=pattern,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_COLUMNS,\n",
        "        column_defaults=DEFAULTS)\n",
        "\n",
        "    # Map dataset to features and label\n",
        "    dataset = dataset.map(map_func=features_and_labels)  # features, label\n",
        "\n",
        "    # Shuffle and repeat for training\n",
        "    if mode == 'train':\n",
        "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
        "\n",
        "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
        "    dataset = dataset.prefetch(buffer_size=1)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def create_input_layers():\n",
        "    \"\"\"Creates dictionary of input layers for each feature.\n",
        "    Returns:\n",
        "        Dictionary of `tf.Keras.layers.Input` layers for each feature.\n",
        "    \"\"\"\n",
        "    deep_inputs = {\n",
        "        colname: tf.keras.layers.Input(\n",
        "            name=colname, shape=(), dtype=\"float32\")\n",
        "        for colname in [\"mother_age\", \"gestation_weeks\"]\n",
        "    }\n",
        "\n",
        "    wide_inputs = {\n",
        "        colname: tf.keras.layers.Input(\n",
        "            name=colname, shape=(), dtype=\"string\")\n",
        "        for colname in [\"is_male\", \"mother_married\", \"plurality\"]\n",
        "    }\n",
        "\n",
        "    inputs = {**wide_inputs, **deep_inputs}\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def categorical_fc(name, values):\n",
        "    \"\"\"Helper function to wrap categorical feature by indicator column.\n",
        "    Args:\n",
        "        name: str, name of feature.\n",
        "        values: list, list of strings of categorical values.\n",
        "    Returns:\n",
        "        Categorical and indicator column of categorical feature.\n",
        "    \"\"\"\n",
        "    cat_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "            key=name, vocabulary_list=values)\n",
        "    ind_column = tf.feature_column.indicator_column(\n",
        "        categorical_column=cat_column)\n",
        "\n",
        "    return cat_column, ind_column\n",
        "\n",
        "\n",
        "def create_feature_columns(nembeds):\n",
        "    \"\"\"Creates wide and deep dictionaries of feature columns from inputs.\n",
        "    Args:\n",
        "        nembeds: int, number of dimensions to embed categorical column down to.\n",
        "    Returns:\n",
        "        Wide and deep dictionaries of feature columns.\n",
        "    \"\"\"\n",
        "    deep_fc = {\n",
        "        colname: tf.feature_column.numeric_column(key=colname)\n",
        "        for colname in [\"mother_age\", \"gestation_weeks\"]\n",
        "    }\n",
        "    wide_fc = {}\n",
        "    is_male, wide_fc[\"is_male\"] = categorical_fc(\n",
        "        \"is_male\", [\"True\", \"False\", \"Unknown\"])\n",
        "    mother_married, wide_fc[\"mother_married\"] = categorical_fc(\n",
        "        \"mother_married\", [\"True\", \"False\", \"Unknown\"])\n",
        "    plurality, wide_fc[\"plurality\"] = categorical_fc(\n",
        "        \"plurality\", [\"1\", \"2\", \"3\",\n",
        "                      \"4\", \"5\", \"6\" ])\n",
        "\n",
        "    # Bucketize the float fields. This makes them wide\n",
        "    age_buckets = tf.feature_column.bucketized_column(\n",
        "        source_column=deep_fc[\"mother_age\"],\n",
        "        boundaries=np.arange(15, 45, 1).tolist())\n",
        "    wide_fc[\"age_buckets\"] = tf.feature_column.indicator_column(\n",
        "        categorical_column=age_buckets)\n",
        "\n",
        "    gestation_buckets = tf.feature_column.bucketized_column(\n",
        "        source_column=deep_fc[\"gestation_weeks\"],\n",
        "        boundaries=np.arange(17, 47, 1).tolist())\n",
        "    wide_fc[\"gestation_buckets\"] = tf.feature_column.indicator_column(\n",
        "        categorical_column=gestation_buckets)\n",
        "\n",
        "    # Cross all the wide columns, have to do the crossing before we one-hot\n",
        "    crossed = tf.feature_column.crossed_column(\n",
        "        keys=[age_buckets, gestation_buckets],\n",
        "        hash_bucket_size=1000)\n",
        "    deep_fc[\"crossed_embeds\"] = tf.feature_column.embedding_column(\n",
        "        categorical_column=crossed, dimension=nembeds)\n",
        "\n",
        "    return wide_fc, deep_fc\n",
        "\n",
        "\n",
        "def get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units):\n",
        "    \"\"\"Creates model architecture and returns outputs.\n",
        "    Args:\n",
        "        wide_inputs: Dense tensor used as inputs to wide side of model.\n",
        "        deep_inputs: Dense tensor used as inputs to deep side of model.\n",
        "        dnn_hidden_units: List of integers where length is number of hidden\n",
        "            layers and ith element is the number of neurons at ith layer.\n",
        "    Returns:\n",
        "        Dense tensor output from the model.\n",
        "    \"\"\"\n",
        "    # Hidden layers for the deep side\n",
        "    layers = [int(x) for x in dnn_hidden_units]\n",
        "    deep = deep_inputs\n",
        "    for layerno, numnodes in enumerate(layers):\n",
        "        deep = tf.keras.layers.Dense(\n",
        "            units=numnodes,\n",
        "            activation=\"relu\",\n",
        "            name=\"dnn_{}\".format(layerno+1))(deep)\n",
        "    deep_out = deep\n",
        "\n",
        "    # Linear model for the wide side\n",
        "    wide_out = tf.keras.layers.Dense(\n",
        "        units=10, activation=\"relu\", name=\"linear\")(wide_inputs)\n",
        "\n",
        "    # Concatenate the two sides\n",
        "    both = tf.keras.layers.concatenate(\n",
        "        inputs=[deep_out, wide_out], name=\"both\")\n",
        "\n",
        "    # Final output is a linear activation because this is regression\n",
        "    output = tf.keras.layers.Dense(\n",
        "        units=1, activation=\"linear\", name=\"weight\")(both)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    \"\"\"Calculates RMSE evaluation metric.\n",
        "    Args:\n",
        "        y_true: tensor, true labels.\n",
        "        y_pred: tensor, predicted labels.\n",
        "    Returns:\n",
        "        Tensor with value of RMSE between true and predicted labels.\n",
        "    \"\"\"\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
        "\n",
        "\n",
        "def build_wide_deep_model(dnn_hidden_units=[64, 32], nembeds=3):\n",
        "    \"\"\"Builds wide and deep model using Keras Functional API.\n",
        "    Returns:\n",
        "        `tf.keras.models.Model` object.\n",
        "    \"\"\"\n",
        "    # Create input layers\n",
        "    inputs = create_input_layers()\n",
        "\n",
        "    # Create feature columns for both wide and deep\n",
        "    wide_fc, deep_fc = create_feature_columns(nembeds)\n",
        "\n",
        "    # The constructor for DenseFeatures takes a list of numeric columns\n",
        "    # The Functional API in Keras requires: LayerConstructor()(inputs)\n",
        "    wide_inputs = tf.keras.layers.DenseFeatures(\n",
        "        feature_columns=wide_fc.values(), name=\"wide_inputs\")(inputs)\n",
        "    deep_inputs = tf.keras.layers.DenseFeatures(\n",
        "        feature_columns=deep_fc.values(), name=\"deep_inputs\")(inputs)\n",
        "\n",
        "    # Get output of model given inputs\n",
        "    output = get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units)\n",
        "\n",
        "    # Build model and compile it all together\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_and_evaluate(args):\n",
        "    model = build_wide_deep_model(args[\"nnsize\"], args[\"nembeds\"])\n",
        "    print(\"Here is our Wide-and-Deep architecture so far:\\n\")\n",
        "    print(model.summary())\n",
        "\n",
        "    trainds = load_dataset(\n",
        "        args[\"train_data_path\"],\n",
        "        args[\"batch_size\"],\n",
        "        'train')\n",
        "\n",
        "    evalds = load_dataset(\n",
        "        args[\"eval_data_path\"], 1000, 'eval')\n",
        "    if args[\"eval_steps\"]:\n",
        "        evalds = evalds.take(count=args[\"eval_steps\"])\n",
        "\n",
        "    num_batches = args[\"batch_size\"] * args[\"num_epochs\"]\n",
        "    steps_per_epoch = args[\"train_examples\"] // num_batches\n",
        "\n",
        "    checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/babyweight\")\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        trainds,\n",
        "        validation_data=evalds,\n",
        "        epochs=args[\"num_epochs\"],\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
        "        callbacks=[cp_callback])\n",
        "\n",
        "    EXPORT_PATH = os.path.join(\n",
        "        args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "    tf.saved_model.save(\n",
        "        obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
        "\n",
        "    hp_metric = history.history['val_rmse'][-1]\n",
        "\n",
        "    hpt = hypertune.HyperTune()\n",
        "    hpt.report_hyperparameter_tuning_metric(\n",
        "        hyperparameter_metric_tag='rmse',\n",
        "        metric_value=hp_metric,\n",
        "        global_step=args['num_epochs'])\n",
        "\n",
        "    print(\"Exported trained model to {}\".format(EXPORT_PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ./pesobebes/trainer/model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2qpgxHnqoJy",
        "outputId": "062c610e-d034-4a1d-d0a1-242893d12460"
      },
      "source": [
        "%%writefile ./pesobebes/trainer/task.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "\n",
        "from trainer import model\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--job-dir\",\n",
        "        help=\"this model ignores this field, but it is required by gcloud\",\n",
        "        default=\"junk\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_data_path\",\n",
        "        help=\"GCS location of training data\",\n",
        "        required=True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eval_data_path\",\n",
        "        help=\"GCS location of evaluation data\",\n",
        "        required=True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        help=\"GCS location to write checkpoints and export models\",\n",
        "        required=True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--batch_size\",\n",
        "        help=\"Number of examples to compute gradient over.\",\n",
        "        type=int,\n",
        "        default=512\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--nnsize\",\n",
        "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
        "        nargs=\"+\",\n",
        "        type=int,\n",
        "        default=[128, 32, 4]\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--nembeds\",\n",
        "        help=\"Embedding size of a cross of n key real-valued parameters\",\n",
        "        type=int,\n",
        "        default=3\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_epochs\",\n",
        "        help=\"Number of epochs to train the model.\",\n",
        "        type=int,\n",
        "        default=10\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_examples\",\n",
        "        help=\"\"\"Number of examples (in thousands) to run the training job over.\n",
        "        If this is more than actual # of examples available, it cycles through\n",
        "        them. So specifying 1000 here when you have only 100k examples makes\n",
        "        this 10 epochs.\"\"\",\n",
        "        type=int,\n",
        "        default=5000\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eval_steps\",\n",
        "        help=\"\"\"Positive number of steps for which to evaluate model. Default\n",
        "        to None, which means to evaluate until input_fn raises an end-of-input\n",
        "        exception\"\"\",\n",
        "        type=int,\n",
        "        default=None\n",
        "    )\n",
        "\n",
        "    # Parse all arguments\n",
        "    args = parser.parse_args()\n",
        "    arguments = args.__dict__\n",
        "\n",
        "    # Unused args provided by service\n",
        "    arguments.pop(\"job_dir\", None)\n",
        "    arguments.pop(\"job-dir\", None)\n",
        "\n",
        "    # Modify some arguments\n",
        "    arguments[\"train_examples\"] *= 1000\n",
        "\n",
        "    # Append trial_id to path if we are doing hptuning\n",
        "    # This code can be removed if you are not using hyperparameter tuning\n",
        "    arguments[\"output_dir\"] = os.path.join(\n",
        "        arguments[\"output_dir\"],\n",
        "        json.loads(\n",
        "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
        "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
        "    )\n",
        "\n",
        "    # Run the training job\n",
        "    model.train_and_evaluate(arguments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ./pesobebes/trainer/task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5sHRZ8sq1bl"
      },
      "source": [
        "## Cria Task de Treinamento (task.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRbxMhXDqvUk",
        "outputId": "423c71f5-5b1f-43ff-a3a1-3b60d4be374d"
      },
      "source": [
        "%%bash\n",
        "\n",
        "OUTDIR=gs://${BUCKET}/pesobebes/trained_model\n",
        "JOBID=pesobebes_$(date -u +%y%m%d_%H%M%S)\n",
        "\n",
        "gcloud ai-platform jobs submit training ${JOBID} \\\n",
        "    --region=${REGION} \\\n",
        "    --project=${PROJECT} \\\n",
        "    --module-name=trainer.task \\\n",
        "    --package-path=$(pwd)/pesobebes/trainer \\\n",
        "    --job-dir=${OUTDIR} \\\n",
        "    --staging-bucket=gs://${BUCKET} \\\n",
        "    --master-machine-type=n1-standard-8 \\\n",
        "    --scale-tier=CUSTOM \\\n",
        "    --runtime-version=${TFVERSION} \\\n",
        "    --python-version=${PYTHONVERSION} \\\n",
        "    -- \\\n",
        "    --train_data_path=gs://${BUCKET}/pesobebes/data/treino*.csv \\\n",
        "    --eval_data_path=gs://${BUCKET}/pesobebes/data/teste*.csv \\\n",
        "    --output_dir=${OUTDIR} \\\n",
        "    --num_epochs=10 \\\n",
        "    --train_examples=10000 \\\n",
        "    --eval_steps=100 \\\n",
        "    --batch_size=32 \\\n",
        "    --nembeds=8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jobId: pesobebes_210823_021424\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Job [pesobebes_210823_021424] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs describe pesobebes_210823_021424\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs stream-logs pesobebes_210823_021424\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkUyXxdarlsY"
      },
      "source": [
        "## INTERVALO!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU1cKZJIrY-C"
      },
      "source": [
        "<font color='red'> Este processo deve demorar de 15 a 20 minutos! Hora do intervalo!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJoDAGa0r_Lz"
      },
      "source": [
        "## Verifique nossos arquivos de modelos treinados\n",
        "\n",
        "Vamos verificar a estrutura de diretório de nossas saídas de nosso modelo treinado na pasta que exportamos.\n",
        "\n",
        "Queremos implementar o saved_model.pb no diretório com timestamp de data / hora, bem como os valores das variáveis na pasta de variáveis. Portanto, precisamos do caminho do diretório para que tudo dentro dele possa ser encontrado pelo serviço de implantação de modelo do Cloud AI Platform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_suczovrHFx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}